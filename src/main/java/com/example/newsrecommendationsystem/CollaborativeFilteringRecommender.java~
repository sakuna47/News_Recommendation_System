package com.example.newsrecommendationsystem;

import com.mongodb.client.MongoClient;
import com.mongodb.client.MongoClients;
import com.mongodb.client.MongoCollection;
import com.mongodb.client.MongoDatabase;
import org.apache.spark.ml.recommendation.ALS;
import org.apache.spark.ml.recommendation.ALSModel;
import org.apache.spark.sql.*;
import org.apache.spark.sql.types.*;
import org.bson.Document;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;

import static org.apache.spark.sql.functions.col;

public class CollaborativeFilteringRecommender {

    private static final String DATABASE_NAME = "CwOOD";
    private static final String INTERACTIONS_COLLECTION = "ArticleInteractions";
    private static final String ARTICLES_COLLECTION = "Articles";

    private final MongoClient mongoClient;
    private final MongoDatabase database;

    public CollaborativeFilteringRecommender() {
        mongoClient = MongoClients.create("mongodb://localhost:27017"); // Update the connection string if necessary
        database = mongoClient.getDatabase(DATABASE_NAME);
    }

    public List<Document> recommendArticles(String username) {
        // Initialize Spark Session
        SparkSession spark = SparkSession.builder()
                .appName("CollaborativeFilteringRecommender")
                .master("local[*]")
                .getOrCreate();

        try {
            // Fetch interactions from MongoDB
            List<Document> userInteractions = fetchInteractionsFromMongo();
            if (userInteractions.isEmpty()) {
                System.out.println("No interactions found for recommendation.");
                return new ArrayList<>();
            }

            // Create DataFrame for interactions
            Dataset<Row> interactionsDF = createInteractionsDataFrame(spark, userInteractions);

            // Build and train ALS model
            ALSModel model = trainALSModel(interactionsDF);

            // Predict ratings for the given user
            Dataset<Row> userPredictions = predictForUser(spark, model, username);

            // Fetch recommended articles from MongoDB
            List<Document> recommendedArticles = fetchArticlesFromPredictions(userPredictions);

            return recommendedArticles;
        } finally {
            spark.stop();
        }
    }

    private List<Document> fetchInteractionsFromMongo() {
        MongoCollection<Document> interactionsCollection = database.getCollection(INTERACTIONS_COLLECTION);
        return interactionsCollection.find().into(new ArrayList<>());
    }

    private Dataset<Row> createInteractionsDataFrame(SparkSession spark, List<Document> interactions) {
        List<Row> rows = new ArrayList<>();
        for (Document interaction : interactions) {
            String username = interaction.getString("username");
            int articleId = interaction.getInteger("articleId");
            int rating = interaction.getInteger("rating");
            rows.add(RowFactory.create(username, articleId, rating));
        }

        StructType schema = new StructType()
                .add("username", DataTypes.StringType, false)
                .add("articleId", DataTypes.IntegerType, false)
                .add("rating", DataTypes.IntegerType, false);

        return spark.createDataFrame(rows, schema);
    }

    private ALSModel trainALSModel(Dataset<Row> interactionsDF) {
        ALS als = new ALS()
                .setUserCol("username")
                .setItemCol("articleId")
                .setRatingCol("rating")
                .setMaxIter(10)
                .setRegParam(0.1)
                .setRank(10)
                .setColdStartStrategy("drop"); // Drop NaN predictions

        return als.fit(interactionsDF);
    }

    private Dataset<Row> predictForUser(SparkSession spark, ALSModel model, String username) {
        // Get unique article IDs for all articles
        MongoCollection<Document> articlesCollection = database.getCollection(ARTICLES_COLLECTION);
        List<Document> articles = articlesCollection.find().into(new ArrayList<>());

        List<Row> articleRows = new ArrayList<>();
        for (Document article : articles) {
            int articleId = article.getInteger("articleId");
            articleRows.add(RowFactory.create(username, articleId));
        }

        StructType schema = new StructType()
                .add("username", DataTypes.StringType, false)
                .add("articleId", DataTypes.IntegerType, false);

        Dataset<Row> userArticlePairs = spark.createDataFrame(articleRows, schema);

        // Predict ratings for user-article pairs
        return model.transform(userArticlePairs).filter(col("prediction").isNotNull())
                .orderBy(col("prediction").desc());
    }

    private List<Document> fetchArticlesFromPredictions(Dataset<Row> predictions) {
        MongoCollection<Document> articlesCollection = database.getCollection(ARTICLES_COLLECTION);
        List<Document> recommendedArticles = new ArrayList<>();

        // Convert predicted article IDs into a list
        List<Integer> recommendedArticleIds = predictions.select("articleId")
                .as(Encoders.INT())
                .collectAsList();

        // Fetch articles by ID from MongoDB
        for (int articleId : recommendedArticleIds) {
            Document article = articlesCollection.find(new Document("articleId", articleId)).first();
            if (article != null) {
                recommendedArticles.add(article);
            }
        }

        return recommendedArticles;
    }

    public void close() {
        mongoClient.close();
    }

    public static void main(String[] args) {
        CollaborativeFilteringRecommender recommender = new CollaborativeFilteringRecommender();

        try {
            // Replace with the actual username
            String username = "sakuna@gmail.com";

            // Get recommended articles
            List<Document> recommendedArticles = recommender.recommendArticles(username);

            // Print recommended articles
            System.out.println("Recommended Articles:");
            for (Document article : recommendedArticles) {
                System.out.println(article.getString("title") + " - Category: " + article.getString("category"));
            }
        } finally {
            recommender.close();
        }
    }
}
